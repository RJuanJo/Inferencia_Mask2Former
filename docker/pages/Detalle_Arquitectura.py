import streamlit as st
from PIL import Image
import os

st.set_page_config(page_title="Detalle de la Arquitectura", layout="wide")

st.title("Funcionamiento de Mask2Former para Segmentaci√≥n de Instancias")

st.markdown("### Introducci√≥n General")
st.markdown("""
Mask2Former es una arquitectura basada en transformers dise√±ada espec√≠ficamente para segmentaci√≥n **pan√≥ptica**, **de instancias** y **sem√°ntica**.  
Se basa en un mecanismo de atenci√≥n enmascarada que restringe el foco del modelo a regiones relevantes, usando consultas aprendibles (queries) para predecir directamente m√°scaras binarias asociadas a cada clase.

En esta secci√≥n profundizamos en su arquitectura, entradas y salidas, y el funcionamiento interno del mecanismo de atenci√≥n.
""")

# COMPONENTES CLAVE
st.markdown("### 1. Componentes Clave")

with st.expander("Atenci√≥n Enmascarada (Masked Attention)"):
    st.markdown("""
    - Reemplaza la atenci√≥n global de los transformers tradicionales.  
    - **Enfoca la atenci√≥n solo en regiones de inter√©s** (m√°scaras predichas), evitando distracciones en fondos irrelevantes.  
    - Ejemplo: Si una *query* predice un "perro", la atenci√≥n se limita a la regi√≥n de ese perro.
    """)

with st.expander("Caracter√≠sticas Multi-Escala"):
    st.markdown("""
    - Combina caracter√≠sticas de **alta resoluci√≥n** (1/8 de la imagen original) y **baja resoluci√≥n** (1/32).  
    - Ventaja: Detecta objetos peque√±os con precisi√≥n gracias a detalles finos de alta resoluci√≥n.
    """)

with st.expander("Optimizaciones"):
    st.markdown("""
    - **Orden de capas**: Cambia el orden de las capas de auto-atenci√≥n y atenci√≥n cruzada.  
    - **Queries aprendibles**: Inicializa las queries como par√°metros entrenables (no ceros).  
    - **Sin dropout**: Mejora el rendimiento al remover regularizaci√≥n innecesaria.
    """)

with st.expander("P√©rdida Eficiente"):
    st.markdown("""
    - Calcula la p√©rdida en puntos muestreados aleatoriamente (ej: 112√ó112 puntos), reduciendo consumo de memoria.
    """)

# ENTRADAS Y SALIDAS
st.markdown("### 2. Entradas y Salidas del Modelo")

st.markdown("""
**Entradas**  
- Imagen: Tensor `[H, W, 3]`.  
- Queries: `[N, C]`, vectores aprendibles inicializados aleatoriamente que act√∫an como propuestas de instancia.

**Salidas**  
- M√°scaras binarias: `[N, H/4, W/4]`.  
- Scores de clases: `[N, K]`.
""")
st.image("sources/entradas_salidas.jpg", caption="Entradas: imagen + queries; Salidas: m√°scaras y scores", use_container_width=True)

# Q, K, V
st.markdown("### 3. Generaci√≥n de Q, K y V")

st.markdown("""
**Queries (Q)**  
- Aprendibles, refinadas por el decodificador transformer en m√∫ltiples capas.

**Keys y Values (K, V)**  
- Derivados del Pixel Decoder que genera una pir√°mide de caracter√≠sticas multi-escala (resoluciones 1/32, 1/16, 1/8).  

```python
K = Linear(features + pos_emb + scale_emb)
V = Linear(features + pos_emb + scale_emb)
````

""")
st.image("sources/vectores_qkv.jpg", caption="Generaci√≥n de vectores Q (consultas), K (claves), V (valores)", use_container_width=True)

# CREACI√ìN DE M√ÅSCARAS

st.markdown("### 4. Creaci√≥n Iterativa de M√°scaras")

st.markdown("""

```python
for l in range(L):  
    Q = Q_prev + masked_attention(Q_prev, K, V)  
    Q = Q + self_attention(Q)  
    Q = Q + FFN(Q)  
    M_l = Linear(Q) + upsample(M_l-1)
```

Cada capa refina las m√°scaras predichas. Se construyen de forma progresiva con atenci√≥n enmascarada, auto-atenci√≥n y FFN.
""")
st.image("sources/creacion_mascaras.jpg", caption="Proceso de refinamiento iterativo de m√°scaras", use_container_width=True)

# ATENCI√ìN ENMASCARADA

st.markdown("### 5. Mecanismo de Atenci√≥n Enmascarada")

st.markdown("""
Cada capa del decodificador toma como entrada una m√°scara binaria de la capa anterior (`M‚Çó‚Çã‚ÇÅ`) y enfoca la atenci√≥n solo dentro de esa regi√≥n:

```math
{Attention}(Q, K, V) = \\text{softmax}(\\bm{\\mathcal{M}}_{l-1} + QK^T/\\sqrt{d})V
```

* ùìú(x,y) = 0 si M‚Çó‚Çã‚ÇÅ(x,y) = 1 (dentro de la ROI).
* ùìú(x,y) = -‚àû si M‚Çó‚Çã‚ÇÅ(x,y) = 0 (excluye fondos).
  """)
st.image("sources/atencion_enmascarada.jpg", caption="Atenci√≥n concentrada solo en regiones relevantes", use_container_width=True)

# DIFERENCIAS

st.markdown("### 6. Diferencias con Modelos Anteriores")
st.image("sources/tabla_diferencias.jpg", use_container_width=True)

# POR QU√â IDEAL

st.markdown("### 7. ¬øPor qu√© es ideal para segmentaci√≥n de instancias?")
st.markdown("""

* **Precisi√≥n en bordes**: mejora la calidad del contorno.
* **Separaci√≥n de objetos solapados**: atenci√≥n localizada por instancia.
* **Reconocimiento de objetos peque√±os**: gracias a las caracter√≠sticas de alta resoluci√≥n.
  """)

# PROCESO PASO A PASO
st.markdown("### 8. Proceso Paso a Paso de Segmentaci√≥n")
st.markdown("""
A continuaci√≥n se detalla c√≥mo Mask2Former procesa una imagen para generar m√°scaras de instancias:
""")

st.markdown("""
#### **Paso 1: Inicializaci√≥n**  
- **Entrada**: El modelo recibe una imagen (ej: foto con perros y √°rboles).  
- **100 queries**: Vectores aprendibles que act√∫an como "notas adhesivas vac√≠as" para registrar informaci√≥n de objetos.  
  - Cada query se especializa en un objeto distinto (Query 1 ‚Üí Perro 1, Query 2 ‚Üí Perro 2, etc.).  
- **Detalle t√©cnico**: Estas queries son par√°metros entrenables que el modelo ajusta durante el aprendizaje.  
""")

st.markdown("""
#### **Paso 2: Atenci√≥n Enmascarada**  
- Cada query analiza solo la regi√≥n de la imagen donde predijo un objeto en el paso anterior.  
  - **Ejemplo**: Si una query identific√≥ un perro, ignora √°rboles, cielo y otros elementos.  
- **Innovaci√≥n clave**:  
  - Modelos antiguos (como DETR) analizan toda la imagen para cada query.  
  - Mask2Former usa m√°scaras binarias para restringir el √°rea de atenci√≥n.  
""")

st.markdown("""
#### **Paso 3: Generaci√≥n de M√°scaras y Clases**  
Cada query produce:  
1. **M√°scara binaria**:  
   - Mapa de p√≠xeles donde `1` = objeto y `0` = fondo.  
   - Resoluci√≥n: 1/4 del tama√±o original (balance precisi√≥n-eficiencia).  
2. **Clase y confianza**:  
   - Predicci√≥n categ√≥rica (ej: "perro" con 95% de confianza).  
   - Si no detecta un objeto, devuelve "no objeto".  
""")

st.markdown("""
#### **Paso 4: Refinamiento (9 Iteraciones)**  
El proceso se repite 9 veces (3 grupos de 3 capas) para mejorar las m√°scaras:  

| **Capas** | **Resoluci√≥n** | **Enfoque**                                  |  
|-----------|----------------|---------------------------------------------|  
| 1-3       | 1/32 (baja)    | Contexto general (ej: "hay un perro").      |  
| 4-6       | 1/16 (media)   | Formas b√°sicas (ej: "4 patas y cola").      |  
| 7-9       | 1/8 (alta)     | Detalles finos (ej: "orejas puntiagudas").  |  

**¬øPor qu√© 9 iteraciones?**  
- **Jerarqu√≠a de caracter√≠sticas**: Combina contexto global (capas iniciales) con detalles precisos (capas finales).  
- **Eficiencia**: 9 capas demostraron ser el equilibrio √≥ptimo entre precisi√≥n y coste computacional.  
""")

# RESULTADOS Y REFERENCIAS

st.markdown("### 9. Resultados y Referencias")
st.markdown("""
**Resultados Clave**  
- **COCO Instance Segmentation**: 50.1 AP (supera HTC++).  
- **Eficiencia**: Solo 50 √©pocas para alcanzar rendimiento de alto nivel.

**Referencias Oficiales**

- [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) ‚Äì Paper oficial.  
- [Repositorio GitHub oficial](https://github.com/facebookresearch/Mask2Former.git)  
- [Modelo en Hugging Face](https://huggingface.co/facebook/mask2former-swin-large-coco-panoptic)

**Cr√©ditos**  
Todas las explicaciones, diagramas e ilustraciones fueron construidas con base en el contenido del paper oficial y sus recursos asociados.  
**Todo el cr√©dito por la arquitectura y avances t√©cnicos es de los autores originales. Este proyecto es una interfaz educativa/demostrativa sin ninguna autor√≠a sobre el modelo ni sus fundamentos.**
""")
