import streamlit as st
from PIL import Image
import os

st.set_page_config(page_title="Detalle de la Arquitectura", layout="wide")

st.title("Funcionamiento de Mask2Former para Segmentaci√≥n Sem√°ntica")  # Cambiado

st.markdown("### Introducci√≥n General")
st.markdown("""
Mask2Former es una arquitectura basada en transformers dise√±ada para segmentaci√≥n **sem√°ntica**, **pan√≥ptica** y **de instancias**.  
Su mecanismo de atenci√≥n enmascarada permite enfocarse en regiones relevantes de cada categor√≠a, usando consultas aprendibles (queries) para predecir m√°scaras por clase (no por objeto individual).

En esta secci√≥n exploramos c√≥mo se adapta para segmentaci√≥n sem√°ntica.
""")

# COMPONENTES CLAVE
st.markdown("### 1. Componentes Clave")

with st.expander("Atenci√≥n Enmascarada (Masked Attention)"):
    st.markdown("""
    - Reemplaza la atenci√≥n global de los transformers tradicionales.  
    - **Enfoca la atenci√≥n solo en p√≠xeles de inter√©s para cada clase** (ej: si una query representa "√°rboles", ignora edificios o calles).  
    - Ejemplo: Query "carretera" ‚Üí atenci√≥n limitada a p√≠xeles de vialidad.
    """)

with st.expander("Caracter√≠sticas Multi-Escala"):
    st.markdown("""
    - Combina caracter√≠sticas de **alta resoluci√≥n** (1/8) y **baja resoluci√≥n** (1/32).  
    - Ventaja: Detecta categor√≠as peque√±as (ej: se√±ales de tr√°fico) con precisi√≥n.
    """)

with st.expander("Optimizaciones"):
    st.markdown("""
    - **Queries aprendibles**: Cada query se especializa en una clase (ej: Query 1 = "cielo", Query 2 = "vegetaci√≥n").  
    - **Sin dropout**: Mejora rendimiento en tareas sem√°nticas donde la coherencia espacial es cr√≠tica.
    """)

# ENTRADAS Y SALIDAS
st.markdown("### 2. Entradas y Salidas del Modelo")
st.markdown("""
**Entradas**  
- Imagen: Tensor `[H, W, 3]`.  
- Queries: `[N, C]`, vectores aprendibles asociados a categor√≠as.

**Salidas**  
- Mapa de clases: `[H/4, W/4]` (cada p√≠xel tiene un ID de clase).  
- Scores por categor√≠a: `[N, K]` (confianza por clase).
""")
st.image("sources/entradas_salidas.jpg", caption="Entradas: imagen + queries; Salidas: mapa de clases y scores", use_column_width=True)  # Leyenda ajustada

# Q, K, V
st.markdown("### 3. Generaci√≥n de Q, K y V")
st.markdown("""
**Queries (Q)**  
- Aprendibles, cada una representa una categor√≠a sem√°ntica (ej: "edificios", "peatones").

**Keys y Values (K, V)**  
- Derivados del Pixel Decoder con caracter√≠sticas multi-escala.  
```python
K = Linear(features + pos_emb + scale_emb)
V = Linear(features + pos_emb + scale_emb)
```
""")
st.image("sources/vectores_qkv.jpg", caption="Generaci√≥n de Q (clases), K, V (caracter√≠sticas)", use_column_width=True)

# CREACI√ìN DE M√ÅSCARAS
st.markdown("### 4. Creaci√≥n Iterativa del Mapa Sem√°ntico")
st.markdown("""
```python
for l in range(L):  
    Q = Q_prev + masked_attention(Q_prev, K, V)  # Atenci√≥n por clase
    M_l = Linear(Q) + upsample(M_l-1)  # Refina m√°scaras de categor√≠as
```
""")
st.image("sources/creacion_mascaras.jpg", caption="Refinamiento progresivo del mapa de clases", use_column_width=True)

# ATENCI√ìN ENMASCARADA
st.markdown("### 5. Atenci√≥n Enmascarada por Clase")
st.markdown("""
Cada query restringe la atenci√≥n a p√≠xeles de su categor√≠a:
```math
{Attention}(Q, K, V) = \\text{softmax}(\\bm{\\mathcal{M}}_{l-1} + QK^T/\\sqrt{d})V
```
* ùìú(x,y) = 0 si el p√≠xel pertenece a la clase actual.  
* ùìú(x,y) = -‚àû si no es relevante para la query.
""")
st.image("sources/atencion_enmascarada.jpg", caption="Atenci√≥n enfocada en p√≠xeles de la clase objetivo", use_column_width=True)

# DIFERENCIAS
st.markdown("### 6. Diferencias con Modelos Anteriores")
st.image("sources/tabla_diferencias.jpg", use_column_width=True)
st.markdown("""
**Nota**:  
- Las queries agrupan p√≠xeles por categor√≠a.  
- Las m√°scaras son mapas por clase (ej: todos los "√°rboles" en una sola regi√≥n).
""")

# POR QU√â IDEAL
st.markdown("### 7. ¬øPor qu√© es ideal para segmentaci√≥n sem√°ntica?")
st.markdown("""
* **Precisi√≥n en bordes**: Define l√≠mites entre clases (ej: acera vs. calle).  
* **Consistencia espacial**: Mantiene coherencia en √°reas grandes (ej: cielo).  
* **Eficiencia**: Menos queries necesarias vs. segmentaci√≥n de instancias.
""")

# PROCESO PASO A PASO
st.markdown("### 8. Proceso Paso a Paso")
st.markdown("""
#### **Paso 1: Inicializaci√≥n**  
- **Queries**: Cada una representa una clase (ej: Query 1 = "veh√≠culos", Query 2 = "peatones").  

#### **Paso 2: Atenci√≥n Enmascarada**  
- Cada query analiza solo p√≠xeles de su categor√≠a (ignora fondos irrelevantes).  

#### **Paso 3: Salidas**  
1. **Mapa de clases**: `[H/4, W/4]` (asignaci√≥n por p√≠xel).  
2. **Confianza por clase**: Probabilidad global de cada categor√≠a.  

#### **Paso 4: Refinamiento (9 Iteraciones)**  
| **Capas** | **Resoluci√≥n** | **Enfoque**                                  |  
|-----------|----------------|---------------------------------------------|  
| 1-3       | 1/32           | Contexto global (ej: "√°rea urbana").        |  
| 4-6       | 1/16           | Estructuras (ej: "formas de edificios").    |  
| 7-9       | 1/8            | Detalles (ej: "ventanas", "se√±alizaci√≥n").  |  
""")

# RESULTADOS
st.markdown("### 9. Resultados en Sem√°ntica")
st.markdown("""
**Benchmarks clave**:  
- **ADE20K**: 57.7 mIoU (state-of-the-art)  
- **Cityscapes**: 84.3 mIoU  
- **COCO-Stuff**: 45.2 mIoU  

**Ventajas**:  
- Preserva bordes n√≠tidos entre categor√≠as.  
- Eficiente para escenas con muchas clases.

**Referencias Oficiales**

- [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) ‚Äì Paper oficial.  
- [Repositorio GitHub oficial](https://github.com/facebookresearch/Mask2Former.git)  
- [Modelo en Hugging Face](https://huggingface.co/facebook/mask2former-swin-large-coco-panoptic)

**Cr√©ditos**  
Todas las explicaciones, diagramas e ilustraciones fueron construidas con base en el contenido del paper oficial y sus recursos asociados.  
**Todo el cr√©dito por la arquitectura y avances t√©cnicos es de los autores originales. Este proyecto es una interfaz educativa/demostrativa sin ninguna autor√≠a sobre el modelo ni sus fundamentos.**
""")