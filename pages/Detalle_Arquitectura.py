import streamlit as st
from PIL import Image
import os

st.set_page_config(page_title="Detalle de la Arquitectura", layout="wide")

st.title("Funcionamiento de Mask2Former para Segmentaci√≥n de Instancias")

st.markdown("### Introducci√≥n General")
st.markdown("""
Mask2Former es una arquitectura basada en transformers dise√±ada espec√≠ficamente para segmentaci√≥n **pan√≥ptica**, **de instancias** y **sem√°ntica**.  
Se basa en un mecanismo de atenci√≥n enmascarada que restringe el foco del modelo a regiones relevantes, usando consultas aprendibles (queries) para predecir directamente m√°scaras binarias asociadas a cada clase.

En esta secci√≥n profundizamos en su arquitectura, entradas y salidas, y el funcionamiento interno del mecanismo de atenci√≥n.
""")

# COMPONENTES CLAVE
st.markdown("### 1. Componentes Clave")

with st.expander("Atenci√≥n Enmascarada (Masked Attention)"):
    st.markdown("""
    - Reemplaza la atenci√≥n global de los transformers tradicionales.  
    - **Enfoca la atenci√≥n solo en regiones de inter√©s** (m√°scaras predichas), evitando distracciones en fondos irrelevantes.  
    - Ejemplo: Si una *query* predice un "perro", la atenci√≥n se limita a la regi√≥n de ese perro.
    """)

with st.expander("Caracter√≠sticas Multi-Escala"):
    st.markdown("""
    - Combina caracter√≠sticas de **alta resoluci√≥n** (1/8 de la imagen original) y **baja resoluci√≥n** (1/32).  
    - Ventaja: Detecta objetos peque√±os con precisi√≥n gracias a detalles finos de alta resoluci√≥n.
    """)

with st.expander("Optimizaciones"):
    st.markdown("""
    - **Orden de capas**: Cambia el orden de las capas de auto-atenci√≥n y atenci√≥n cruzada.  
    - **Queries aprendibles**: Inicializa las queries como par√°metros entrenables (no ceros).  
    - **Sin dropout**: Mejora el rendimiento al remover regularizaci√≥n innecesaria.
    """)

with st.expander("P√©rdida Eficiente"):
    st.markdown("""
    - Calcula la p√©rdida en puntos muestreados aleatoriamente (ej: 112√ó112 puntos), reduciendo consumo de memoria.
    """)

# ENTRADAS Y SALIDAS
st.markdown("### 2. Entradas y Salidas del Modelo")

st.markdown("""
**Entradas**  
- Imagen: Tensor `[H, W, 3]`.  
- Queries: `[N, C]`, vectores aprendibles inicializados aleatoriamente que act√∫an como propuestas de instancia.

**Salidas**  
- M√°scaras binarias: `[N, H/4, W/4]`.  
- Scores de clases: `[N, K]`.
""")
st.image("sources/entradas_salidas.jpg", caption="Entradas: imagen + queries; Salidas: m√°scaras y scores", use_column_width=True)

# Q, K, V
st.markdown("### 3. Generaci√≥n de Q, K y V")

st.markdown("""
**Queries (Q)**  
- Aprendibles, refinadas por el decodificador transformer en m√∫ltiples capas.

**Keys y Values (K, V)**  
- Derivados del Pixel Decoder que genera una pir√°mide de caracter√≠sticas multi-escala (resoluciones 1/32, 1/16, 1/8).  

```python
K = Linear(features + pos_emb + scale_emb)
V = Linear(features + pos_emb + scale_emb)
````

""")
st.image("sources/vectores_qkv.jpg", caption="Generaci√≥n de vectores Q (consultas), K (claves), V (valores)", use_column_width=True)

# CREACI√ìN DE M√ÅSCARAS

st.markdown("### 4. Creaci√≥n Iterativa de M√°scaras")

st.markdown("""

```python
for l in range(L):  
    Q = Q_prev + masked_attention(Q_prev, K, V)  
    Q = Q + self_attention(Q)  
    Q = Q + FFN(Q)  
    M_l = Linear(Q) + upsample(M_l-1)
```

Cada capa refina las m√°scaras predichas. Se construyen de forma progresiva con atenci√≥n enmascarada, auto-atenci√≥n y FFN.
""")
st.image("sources/creacion_mascaras.jpg", caption="Proceso de refinamiento iterativo de m√°scaras", use_column_width=True)

# ATENCI√ìN ENMASCARADA

st.markdown("### 5. Mecanismo de Atenci√≥n Enmascarada")

st.markdown("""
Cada capa del decodificador toma como entrada una m√°scara binaria de la capa anterior (`M‚Çó‚Çã‚ÇÅ`) y enfoca la atenci√≥n solo dentro de esa regi√≥n:

```math
\\text{Attention}(Q, K, V) = \\text{softmax}(\\bm{\\mathcal{M}}_{l-1} + QK^T/\\sqrt{d})V
```

* ùìú(x,y) = 0 si M‚Çó‚Çã‚ÇÅ(x,y) = 1 (dentro de la ROI).
* ùìú(x,y) = -‚àû si M‚Çó‚Çã‚ÇÅ(x,y) = 0 (excluye fondos).
  """)
st.image("sources/atencion_enmascarada.jpg", caption="Atenci√≥n concentrada solo en regiones relevantes", use_column_width=True)

# DIFERENCIAS

st.markdown("### 6. Diferencias con Modelos Anteriores")
st.image("sources/tabla_diferencias.jpg", use_column_width=True)

# POR QU√â IDEAL

st.markdown("### 7. ¬øPor qu√© es ideal para segmentaci√≥n de instancias?")
st.markdown("""

* **Precisi√≥n en bordes**: mejora la calidad del contorno (AP de borde = 36.2 en COCO).
* **Separaci√≥n de objetos solapados**: atenci√≥n localizada por instancia.
* **Reconocimiento de objetos peque√±os**: gracias a las caracter√≠sticas de alta resoluci√≥n.
  """)

# RESULTADOS Y REFERENCIAS

st.markdown("### 8. Resultados y Referencias")
st.markdown("""
**Resultados Clave**  
- **COCO Instance Segmentation**: 50.1 AP (supera HTC++).  
- **Eficiencia**: Solo 50 √©pocas para alcanzar rendimiento de alto nivel.

**Referencias Oficiales**

- [Masked-attention Mask Transformer for Universal Image Segmentation](https://arxiv.org/abs/2112.01527) ‚Äì Paper oficial.  
- [Repositorio GitHub oficial](https://github.com/facebookresearch/Mask2Former.git)  
- [Modelo en Hugging Face](https://huggingface.co/facebook/mask2former-swin-large-coco-panoptic)

**Cr√©ditos**  
Todas las explicaciones, diagramas e ilustraciones fueron construidas con base en el contenido del paper oficial y sus recursos asociados.  
**Todo el cr√©dito por la arquitectura y avances t√©cnicos es de los autores originales. Este proyecto es una interfaz educativa/demostrativa sin ninguna autor√≠a sobre el modelo ni sus fundamentos.**
""")

